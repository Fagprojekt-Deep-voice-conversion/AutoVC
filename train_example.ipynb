{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded encoder \"models/SpeakerEncoder/SpeakerEncoder.pt\" trained to step 1564501\n",
      "Training beginning on cpu\n",
      "Step: 1\n",
      "Step: 2\n",
      "Step: 3\n",
      "Step: 4\n"
     ]
    }
   ],
   "source": [
    "from autovc.utils.dataloader import TrainDataLoader\n",
    "from autovc.utils.model_loader import load_models\n",
    "# from autovc.auto_encoder.model_vc import Generator\n",
    "# from autovc.speaker_encoder.model import SpeakerEncoder\n",
    "# model = Generator()\n",
    "# model.load_model('models/AutoVC/AutoVC_SMK.pt', device = 'cpu')\n",
    "\n",
    "# speaker_encoder = SpeakerEncoder()\n",
    "# speaker_encoder.load_model('models/SpeakerEncoder/SpeakerEncoder.pt')\n",
    "\n",
    "model, speaker_encoder = load_models(\n",
    "    model_types= [\"auto_encoder\", \"speaker_encoder\"],\n",
    "    model_paths= ['models/AutoVC/AutoVC_SMK.pt', 'models/SpeakerEncoder/SpeakerEncoder.pt']\n",
    ")\n",
    "\n",
    "dataset = TrainDataLoader(data_dir_path = 'data/samples', speaker_encoder = speaker_encoder)\n",
    "dataloader = dataset.get_dataloader(batch_size = 2, shuffle = True)\n",
    "\n",
    "model.learn(dataloader, n_epochs = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autovc.speaker_encoder.model import SpeakerEncoder\n",
    "from autovc.speaker_encoder.utils import wav_to_mel_spectrogram, preprocess_wav\n",
    "import torch\n",
    "speaker_encoder = SpeakerEncoder()\n",
    "speaker_encoder.load_model('Models/SpeakerEncoder/SpeakerEncoder.pt')\n",
    "mels = [speaker_encoder(torch.from_numpy(wav_to_mel_spectrogram(preprocess_wav(wav))).unsqueeze(0)) for wav in [\"data/samples/chooped7.wav\", \"data/samples/mette_183.wav\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_encoder.loss(torch.stack(mels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded auto encoder \"models/AutoVC/AutoVC_SMK.pt\" trained to step 13801\n",
      "Loaded speaker encoder \"models/SpeakerEncoder/SpeakerEncoder.pt\" trained to step 0\n",
      "Loaded vocoder \"models/WaveRNN/WaveRNN_Pretrained.pyt\"\n"
     ]
    }
   ],
   "source": [
    "from autovc.utils.audio import get_mel_frames, audio_to_melspectrogram\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autovc.utils.model_loader import load_model\n",
    "from torch.nn.functional import pad\n",
    "import soundfile as sf\n",
    "from autovc.utils.audio import remove_noise\n",
    "AE = load_model('auto_encoder', 'models/AutoVC/AutoVC_SMK.pt')\n",
    "SE = load_model('speaker_encoder', 'models/SpeakerEncoder/SpeakerEncoder.pt')\n",
    "vocoder = load_model('vocoder', 'models/WaveRNN/WaveRNN_Pretrained.pyt')\n",
    "N = 1000\n",
    "source = 'data/samples/hilde_301.wav'\n",
    "target = 'data/samples/HaegueYang_5.wav'\n",
    "min_pad_coverage = 0.1\n",
    "overlap = 0.5\n",
    "frames = get_mel_frames(source,\n",
    "                        audio_to_melspectrogram, \n",
    "                        sr = 22050, \n",
    "                        mel_window_step             = 12.5, \n",
    "                        order                       = 'MF', \n",
    "                        partial_utterance_n_frames  = N, \n",
    "                        min_pad_coverage            = min_pad_coverage, \n",
    "                        overlap                     = overlap,\n",
    "                        )\n",
    "\n",
    "X = torch.stack(frames)\n",
    "c_source = SE.embed_utterance(source).unsqueeze(0).expand(X.size(0),-1)\n",
    "c_target = SE.embed_utterance(target).unsqueeze(0).expand(X.size(0),-1)\n",
    "\n",
    "\n",
    "out, post_out, content_codes = AE(X, c_source, c_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 1000])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "frames = list(post_out)\n",
    "M = len(frames)\n",
    "T = int(N * (1-overlap))\n",
    "for i in range(M):\n",
    "    frames[i] = pad(frames[i], (i * T, (M-i-1) * T) , mode = 'constant', value = torch.nan)\n",
    "\n",
    "X = torch.stack(frames)\n",
    "X_paste = X.nanmean(axis = 0)\n",
    "X_paste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ████████████████ 288000/290400 | Batch Size: 24 | Gen Rate: 6.5kHz | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "waveform = vocoder.generate(X_paste.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wave = np.asarray(waveform)\n",
    "wave = remove_noise(wave, 22050)\n",
    "sf.write('chop_and_past_00p_overlap.wav', wave, samplerate =22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE.state_dict()['test'] = 2\n",
    "\n",
    "torch.save( {'step':0, 'model_state': SE.state_dict(), 'speakers':{'hilde' : torch.randn((1,256)), 'yang':torch.randn((1,256))}}, 'models/SpeakerEncoder/SpeakerEncoder2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded speaker encoder \"models/SpeakerEncoder/SpeakerEncoder2.pt\" trained to step 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_values([tensor([[-0.8979, -0.7079,  0.5696, -0.0379,  1.1863, -0.3241, -0.7413,  0.5706,\n",
       "         -0.8708, -0.7614,  0.8727, -0.3884, -1.1303,  0.2941,  0.5893,  0.0048,\n",
       "          1.6446, -0.5212,  2.1275,  1.0680,  2.3424, -0.2470, -0.5285,  1.2640,\n",
       "          1.0019, -3.6085,  0.5445,  0.0365, -0.2123, -0.0211,  1.4480, -0.1225,\n",
       "         -1.1412, -1.3419,  0.6223, -0.7319,  0.4823,  0.2453, -1.1903,  0.6038,\n",
       "          0.8117,  0.3642, -1.5035,  0.1531, -1.1984,  0.0442,  0.0104, -0.0516,\n",
       "         -0.5144,  1.2197,  1.5099,  0.5336, -1.4649, -0.3608, -0.6761,  1.5573,\n",
       "         -0.3179, -0.0367,  0.3051,  0.0157,  1.5915,  0.3776,  0.8285, -0.6463,\n",
       "          0.2736, -0.4032,  0.0318, -0.7122,  0.6429, -0.6561, -0.1261,  0.4313,\n",
       "          0.2231, -1.3784,  0.5166, -0.9819, -0.1305,  1.5581,  0.1344,  0.1749,\n",
       "         -1.0496,  1.4752,  0.5158, -0.2854,  1.6855,  1.2184, -0.5191,  0.0500,\n",
       "         -0.4805, -1.2850, -2.9035,  0.2492, -0.6376,  1.3687, -0.8934,  1.0843,\n",
       "         -0.3724, -0.5232,  0.6391, -0.0735,  1.8043,  0.0291, -1.7283,  1.0682,\n",
       "         -1.2220,  0.0664,  0.3319, -0.3512, -1.4284, -0.2331, -1.1360,  0.2449,\n",
       "         -1.2844, -1.6463, -2.4417,  0.6425,  1.3726, -1.5420,  0.0928,  0.2383,\n",
       "          0.4081,  0.2094, -1.5703,  0.4025,  1.0625,  0.3813, -0.8496,  0.8990,\n",
       "          0.1871, -0.2145, -0.1494, -0.8333, -0.4000,  0.9578,  0.2204, -1.0571,\n",
       "          1.4808,  0.1510, -1.1424,  0.2886, -0.6734,  1.0364, -0.1346, -1.2616,\n",
       "          0.8163, -1.8108,  2.8483,  0.9381,  1.9848,  0.5429, -0.0278, -0.4554,\n",
       "          0.6317, -0.3068, -1.8449, -1.3343, -1.1682,  0.8313,  1.4992, -0.8938,\n",
       "          1.7029,  0.8943, -0.9644,  0.5693,  2.2288, -0.8866, -0.0173, -0.3293,\n",
       "          0.3596,  0.9519, -2.1474,  0.3595, -0.7667,  0.1257,  0.2985, -0.5020,\n",
       "          1.0953, -0.9892, -1.0777,  0.2057, -1.0053,  1.4462, -0.4710, -1.0577,\n",
       "         -0.1028,  0.0805, -0.5116,  0.4593,  0.0313, -1.2453,  0.0598, -1.8853,\n",
       "         -0.8897, -0.6931,  0.0345,  0.1166, -1.2169, -1.0084, -0.0247, -0.3034,\n",
       "          0.7296, -0.0156,  1.3213,  0.1998, -0.3344,  1.4271,  0.7644, -0.4661,\n",
       "         -0.8900,  0.8116, -0.2795, -2.8631, -0.5154,  0.4689, -1.2349,  0.0664,\n",
       "          0.3654,  0.4304, -0.1326, -0.5288,  0.2848,  2.2255, -1.6301,  0.5588,\n",
       "          0.0919,  1.0482, -0.0878,  1.0843, -2.1942,  1.4032, -0.5679, -1.6112,\n",
       "         -0.7279, -0.3552, -1.4273,  1.0824,  0.5785,  1.0944, -1.0193,  1.4382,\n",
       "         -0.1191,  0.1668, -1.9580,  0.3426,  1.3847, -1.6868, -0.3998, -1.1347,\n",
       "         -0.4927, -1.8674,  0.8296, -1.5776,  1.3863, -0.1812,  1.1892, -0.3492]]), tensor([[-4.7546e-01,  1.1372e+00,  1.1728e+00, -5.0216e-01, -6.1228e-01,\n",
       "          9.2305e-01, -1.9927e-01,  6.4267e-02,  2.3706e-01,  1.9591e-01,\n",
       "         -1.3963e+00, -1.1640e+00, -4.2545e-01, -4.2819e-01,  1.2194e+00,\n",
       "         -2.1065e-01,  8.4636e-02, -7.9041e-01, -1.0547e-01,  1.0224e+00,\n",
       "          3.5846e-01, -1.4423e+00,  1.4599e+00, -7.9032e-02, -1.1940e+00,\n",
       "         -5.9468e-01, -1.4823e+00, -1.7835e+00, -1.0428e+00,  5.1455e-02,\n",
       "         -2.1435e-01,  5.8205e-01,  7.9109e-01, -5.8332e-01, -8.7696e-01,\n",
       "         -1.2564e+00,  1.8670e+00,  2.1812e+00, -3.3531e-01, -3.8825e-01,\n",
       "         -4.2110e-01, -1.1226e+00,  7.7104e-01, -5.1458e-01,  6.2699e-01,\n",
       "          4.8735e-01,  1.7794e+00, -2.7374e-01,  8.6280e-01, -5.4620e-01,\n",
       "         -8.3860e-01, -1.1791e+00, -4.1386e-01, -7.2078e-01,  2.6018e-01,\n",
       "          7.7939e-01,  9.1932e-02, -3.0122e+00, -6.0709e-01,  6.1447e-01,\n",
       "          1.8454e+00, -1.2223e-01, -1.8281e+00, -1.0616e+00,  1.0014e+00,\n",
       "         -4.2858e-01, -2.9538e-01, -8.5815e-02,  1.1226e+00,  5.7804e-01,\n",
       "         -2.0510e+00, -2.2263e+00,  3.1697e-01,  4.3732e-01,  1.3994e+00,\n",
       "         -1.9509e-01, -6.8886e-01, -1.2990e+00,  1.1766e+00, -2.0294e+00,\n",
       "         -7.0581e-01,  2.0375e-01, -7.5025e-01,  4.6016e-01, -4.7244e-01,\n",
       "          3.1051e-01, -3.7862e-01, -5.7523e-01, -1.3345e-01, -6.7202e-01,\n",
       "         -9.5398e-01,  1.2960e-01, -3.9683e-02,  2.9447e-01,  1.8346e+00,\n",
       "          6.0490e-01,  1.3702e+00, -1.0417e+00, -6.4735e-01,  1.3345e+00,\n",
       "         -1.5620e-01, -8.2463e-01,  9.1565e-01, -1.6419e-01, -5.7835e-01,\n",
       "         -1.7755e+00,  8.0279e-01, -5.5162e-01, -9.8063e-01, -4.4594e-01,\n",
       "          2.2006e+00, -3.8108e-01, -1.8783e+00,  1.6502e-01, -9.9151e-01,\n",
       "          4.8257e-01, -6.5811e-01,  9.2466e-01,  9.2893e-01, -1.3947e+00,\n",
       "         -5.3695e-02,  7.1272e-01, -5.8672e-01, -2.2706e+00,  2.1893e-01,\n",
       "          1.1235e+00,  1.2291e+00,  2.3539e-01,  2.1961e-02, -4.8951e-01,\n",
       "          3.4996e-01,  5.6114e-01,  6.8359e-01,  6.6130e-01,  3.1685e-02,\n",
       "          9.8747e-01, -2.2102e-01, -1.2396e+00,  2.3476e-01, -4.2053e-01,\n",
       "         -1.5034e+00,  1.3098e-01,  1.6031e+00, -7.5880e-01, -1.6325e+00,\n",
       "         -3.8999e-01, -1.9508e-01,  8.0719e-02,  6.0006e-01,  1.3736e+00,\n",
       "         -9.4883e-01,  8.6710e-01,  6.4225e-01, -3.8094e-01,  2.5588e+00,\n",
       "          7.5858e-01, -1.7176e-01, -1.0434e-01, -1.1883e-01,  4.1185e-01,\n",
       "         -6.6186e-01, -2.9903e-01, -1.2866e+00,  7.2602e-02, -2.0625e+00,\n",
       "          6.9302e-01, -1.4270e-03,  1.8818e+00,  1.6686e+00,  8.4816e-02,\n",
       "          8.4924e-01,  1.7309e+00,  6.7135e-01, -1.2011e+00,  1.2984e-01,\n",
       "          4.7126e-01,  6.0834e-01,  4.7826e-01, -3.6839e-01, -3.3410e-02,\n",
       "         -2.8372e-01,  1.3693e+00,  2.5202e-01, -3.2927e-02, -1.8866e+00,\n",
       "          3.9712e-02,  5.7547e-01,  2.0491e+00, -5.6768e-01, -9.7612e-01,\n",
       "         -1.1454e-01,  4.5153e-02, -4.7531e-01,  4.8845e-02,  3.7898e-01,\n",
       "          1.8167e+00, -4.6459e-01,  6.3671e-01, -9.5813e-03,  1.7224e+00,\n",
       "         -9.7359e-01,  1.8653e-01,  5.5054e-01, -2.1509e-01, -4.0165e-01,\n",
       "         -1.1024e+00,  8.2501e-03, -2.7722e-01,  1.3655e+00, -1.9869e+00,\n",
       "          1.7691e-01,  2.1743e+00,  1.1683e-01, -7.3857e-02,  1.0077e+00,\n",
       "          6.9847e-02,  3.1629e-01,  1.5358e+00,  1.1540e+00,  7.3666e-01,\n",
       "          2.1790e-01, -1.1354e+00, -1.7120e+00,  9.1880e-01,  7.9826e-01,\n",
       "         -3.0996e-01,  1.9972e+00,  1.5762e+00,  4.1084e-01,  7.2756e-01,\n",
       "         -6.9835e-01,  8.1439e-01,  2.6145e-01, -5.4034e-02, -3.9447e-01,\n",
       "          8.3803e-01, -7.1520e-01, -5.8411e-01, -8.4962e-01, -1.3770e+00,\n",
       "         -4.0426e-01, -2.0239e-01, -1.3997e-02,  4.2465e-02,  1.0579e+00,\n",
       "          1.1079e+00, -6.4251e-01,  1.8003e+00, -7.1456e-02, -1.6395e+00,\n",
       "          8.1223e-01, -4.4183e-01, -1.1940e+00,  1.1870e+00, -3.3282e-01,\n",
       "         -2.7744e-01]])])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SE2 = load_model('speaker_encoder', 'models/SpeakerEncoder/SpeakerEncoder2.pt')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "54e0da04e6ae9c20ca2f4c1b875d1df5d1285f4ba0e46b27cfcdd998c27a32e0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('AutoVC_env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
