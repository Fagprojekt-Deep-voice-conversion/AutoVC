{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autovc.utils.dataloader import SpeakerEncoderDataLoader\n",
    "import torch\n",
    "from autovc.speaker_encoder.model import SpeakerEncoder\n",
    "from autovc.utils.model_loader import load_model\n",
    "import wandb\n",
    "\n",
    "datadir = {'hilde': ['data/hilde_7sek'], 'hague': ['data/HaegueYang_10sek', 'data/hyang_smk']}\n",
    "Data = SpeakerEncoderDataLoader(datadir)\n",
    "\n",
    "\n",
    "SE = load_model('speaker_encoder', 'models/SpeakerEncoder/SpeakerEncoder.pt')\n",
    "run = wandb.init(project = 'SpeakerEncoder',  entity = \"deep_voice_inc\", reinit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autovc.speaker_encoder.model import SpeakerEncoder\n",
    "dataloader = Data.get_dataloader(batch_size = 2)\n",
    "SE = SpeakerEncoder()\n",
    "\n",
    "\n",
    "def batch_forward(batch):\n",
    "    embeddings = []\n",
    "    for b in batch:\n",
    "        embed_speaker = torch.stack([SE.forward(torch.from_numpy(speaker).unsqueeze(0).to('cpu')) for speaker in b])\n",
    "        embeddings.append(embed_speaker)\n",
    "    return torch.cat(embeddings, dim = 1)\n",
    "for i in range(5):\n",
    "    for batch in dataloader:\n",
    "        embeds = batch_forward(batch)\n",
    "        print(embeds.shape)\n",
    "        print(SE.similarity_matrix(embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autovc.utils.model_loader import load_model\n",
    "from autovc.utils.dataloader import SpeakerEncoderDataLoader\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "datadir = {'hilde': ['data/hilde_20211020'], 'hague': ['data/HaegueYang_10sek', 'data/hyang_smk']}\n",
    "Data = SpeakerEncoderDataLoader(datadir)\n",
    "\n",
    "dataloader = Data.get_dataloader(batch_size=264)\n",
    "SE = load_model('speaker_encoder', 'models/SpeakerEncoder/SpeakerEncoder.pt')\n",
    "\n",
    "\n",
    "def batch_forward(batch):\n",
    "\n",
    "    return torch.stack([SE(b) for b in batch])\n",
    "\n",
    "# for i in range(1):\n",
    "#     for batch in dataloader:\n",
    "#         embeds = batch_forward(batch)\n",
    "#         print(embeds.shape)\n",
    "#         X = TSNE(n_components=2 ).fit_transform(torch.flatten(embeds, start_dim  = 0, end_dim = 1).detach().numpy())\n",
    "#         plt.scatter(X[:, 0], X[:, 1])\n",
    "#         plt.show()\n",
    "#         print(SE.loss(embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = Data.get_dataloader(batch_size=3628)\n",
    "for batch in dataloader:\n",
    "    embeds = batch_forward(batch)\n",
    "    print(embeds.shape)\n",
    "    X = TSNE(n_components=2 ).fit_transform(torch.flatten(embeds, start_dim  = 0, end_dim = 1).detach().numpy())\n",
    "    \n",
    "    plt.scatter(X[:len(X)//2,0], X[:len(X)//2,1])\n",
    "    plt.scatter(X[len(X)//2:,0], X[len(X)//2:,1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax.scatter(X[:len(X)//2,0], X[:len(X)//2,1], alpha = 0.6, zorder = 3)\n",
    "ax.scatter(X[len(X)//2:,0], X[len(X)//2:,1], alpha = 0.6, zorder = 3)\n",
    "ax.legend([\"Hilde\", \"Hageueyaa\"])\n",
    "ax.grid(ls = '--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('deep_voice_inc/data/smk_speakers:v0', type='dataset')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autovc.utils.audio import audio_to_melspectrogram, get_mel_frames\n",
    "from autovc.utils.hparams import WaveRNNParams\n",
    "vocoder_params = WaveRNNParams()\n",
    "wav = 'data/samples/chooped7.wav'\n",
    "T = get_mel_frames(wav, audio_to_melspectrogram,  order = 'MF', sr = vocoder_params.sample_rate, mel_window_step = vocoder_params.mel_window_step, partial_utterance_n_frames = 250 )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d339a5673eae03459bd67365dcbf4fbdfa0a00ae370b2bc49c3370bcf7accba8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
