{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from autovc.speaker_encoder.utils import *\n",
    "import soundfile as sf\n",
    "from autovc.utils.hparams import SpeakerEncoderParams as params\n",
    "from autovc.utils.preprocess_wav import audio_to_melspectrogram\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ms183920\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "C:\\Users\\lukas\\miniconda3\\envs\\AutoVC-env\\lib\\site-packages\\IPython\\html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/s183920/AutoVC/runs/2vwiv7vv\" target=\"_blank\">honest-frog-1</a></strong> to <a href=\"https://wandb.ai/s183920/AutoVC\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact smk_speakers:v0, 4947.34MB. 3032 files... Done. 0:0:0\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "# import wandb\n",
    "# run = wandb.init()\n",
    "# artifact = run.use_artifact('deep_voice_inc/data/smk_speakers:v0', type='dataset')\n",
    "# artifact_dir = artifact.download(root=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk = [w for w in os.walk(\"data/yang_test\")][0]\n",
    "root, dirs, files = [w for w in walk]\n",
    "\n",
    "for file in files:\n",
    "    waveform = preprocess_wav(os.path.join(root, file))\n",
    "    sf.write(f\"test_yang_silence/{file}\", np.asarray(waveform), samplerate = 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk = [w for w in os.walk(\"test_yang_silence\")][0]\n",
    "_, _, files = [w for w in walk]\n",
    "\n",
    "specs_org = []\n",
    "specs_new = []\n",
    "\n",
    "for file in files:\n",
    "    specs_org.append(audio_to_melspectrogram(os.path.join(\"data/yang_test\", file)))\n",
    "    specs_new.append(audio_to_melspectrogram(os.path.join(\"test_yang_silence\", file)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186.2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens_new = np.array([np.shape(s)[1] for s in specs_new])\n",
    "lens_org = np.array([np.shape(s)[1] for s in specs_org])\n",
    "\n",
    "\n",
    "# (lens_org-lens_new)/lens_org\n",
    "lens_new.std()\n",
    "lens_new.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.00087523,  0.00051307,  0.00026207, ...,  0.00056371,\n",
       "        -0.00046649, -0.00013715], dtype=float32),\n",
       " array([-0.00150331, -0.00164158, -0.00083389, ..., -0.00511392,\n",
       "        -0.00460699, -0.00470363], dtype=float32),\n",
       " array([-0.00916225, -0.00969645, -0.00811925, ..., -0.00074893,\n",
       "        -0.00157988,  0.00079246], dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = params()\n",
    "\n",
    "def create_audio_mask(wav):\n",
    "    \"\"\"\"\n",
    "    Creates an audio mask, where False indicates silence.\n",
    "\n",
    "    :param wav: a numpy array with the content of a wav file\n",
    "    :return audio_mask: the calculated audio mask\n",
    "    \"\"\"\n",
    "    # Compute the voice detection window size\n",
    "    samples_per_window = (hparams.vad_window_length * hparams.sampling_rate) // 1000\n",
    "    \n",
    "    # Trim the end of the audio to have a multiple of the window size\n",
    "    wav = wav[:len(wav) - (len(wav) % samples_per_window)]\n",
    "    \n",
    "    # Convert the float waveform to 16-bit mono PCM\n",
    "    pcm_wave = struct.pack(\"%dh\" % len(wav), *(np.round(wav * int16_max)).astype(np.int16))\n",
    "    \n",
    "    # Perform voice activation detection\n",
    "    voice_flags = []\n",
    "    vad = webrtcvad.Vad(mode=3)\n",
    "    for window_start in range(0, len(wav), samples_per_window):\n",
    "        window_end = window_start + samples_per_window\n",
    "        voice_flags.append(vad.is_speech(pcm_wave[window_start * 2:window_end * 2],\n",
    "                                         sample_rate=hparams.sampling_rate))\n",
    "    voice_flags = np.array(voice_flags)\n",
    "    \n",
    "    # Smooth the voice detection with a moving average\n",
    "    def moving_average(array, width):\n",
    "        array_padded = np.concatenate((np.zeros((width - 1) // 2), array, np.zeros(width // 2)))\n",
    "        ret = np.cumsum(array_padded, dtype=float)\n",
    "        ret[width:] = ret[width:] - ret[:-width]\n",
    "        return ret[width - 1:] / width\n",
    "    \n",
    "    audio_mask = moving_average(voice_flags, hparams.vad_moving_average_width)\n",
    "    audio_mask = np.round(audio_mask).astype(bool)\n",
    "    \n",
    "    # Dilate the voiced regions\n",
    "    audio_mask = binary_dilation(audio_mask, np.ones(hparams.vad_max_silence_length + 1))\n",
    "    audio_mask = np.repeat(audio_mask, samples_per_window)\n",
    "\n",
    "    return audio_mask\n",
    "\n",
    "def split_audio(wav_path, save_folder = None, allowed_pause = .5):\n",
    "    \"\"\"\n",
    "    Chops the content of the wav file into multiple files, based on when there is\n",
    "    a longer period if silence. Files will be saved with a number indicating the order the content appeared in.\n",
    "\n",
    "    :param wav: path to wav file\n",
    "    :param save_folder: folder to save files in\n",
    "    :allowed_pause: number seconds of silence to allow in a sound file\n",
    "    :return: content of the wav file seperated in multiple files \n",
    "    \"\"\"\n",
    "    # load wav\n",
    "    wav, source_sr = librosa.load(wav_path, sr=None)\n",
    "\n",
    "    audio_mask = create_audio_mask(wav)\n",
    "\n",
    "    # function for finding consecutive values without silence\n",
    "    def consecutive(data, stepsize=1):\n",
    "        return np.split(data, np.where(np.diff(data) != stepsize)[0]+1)\n",
    "\n",
    "\n",
    "    allowed_pause = allowed_pause*source_sr\n",
    "    for i, split in enumerate(consecutive(np.where(audio_mask)[0])):\n",
    "        if i == 0:\n",
    "            joined_splits = [split]\n",
    "        else:\n",
    "            # join last subset with new split if difference is less than allowed pause\n",
    "            if split[-1] - joined_splits[-1][-1] <= allowed_pause:\n",
    "                joined_splits.append(np.concatenate([joined_splits.pop(), split]))\n",
    "            else:\n",
    "                joined_splits.append(split)\n",
    "    \n",
    "    # save chopped files\n",
    "    filename = os.path.split(wav_path)[-1]\n",
    "    wav_splitted = []\n",
    "    if save_folder is not None:\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "        \n",
    "        for i, split in enumerate(joined_splits):\n",
    "            fname = filename.replace(\".wav\", f\"_{str(i+1).zfill(3)}.wav\")\n",
    "            wav_splitted.append(wav[split])\n",
    "            sf.write(f\"{save_folder}/{fname}\", wav_splitted[-1], samplerate = source_sr)\n",
    "\n",
    "\n",
    "    return wav_splitted\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "split_audio(\"data/samples/mette_183.wav\", \"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, False,  True, False, False,  True,  True,\n",
       "       False, False])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = np.array([7040,  7041,  7042, 89597, 89598, 89599])\n",
    "a = np.array([*[False]*4, *[True]*3, False, *[True]*4])\n",
    "# a = np.diff(a)\n",
    "# np.where((~a[:-1])*np.roll(a, 1)[1:])\n",
    "a\n",
    "np.where((a[:-1]==(np.roll(a, 1)[1:])))\n",
    "# np.array_split(wav, mask)\n",
    "# a[:-1]*(np.roll(a, 1)[1:] == False)\n",
    "a[:-1]!=np.roll(a, 2)[1:]\n",
    "# np.roll(a, 2)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7040 50880 72960 84160] [39359 63039 82239 89599]\n"
     ]
    }
   ],
   "source": [
    "print(mins, maxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72960 72961 72962 ... 82237 82238 82239]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 7040,  7041,  7042, ..., 39357, 39358, 39359], dtype=int64),\n",
       " array([50880, 50881, 50882, ..., 63037, 63038, 63039], dtype=int64),\n",
       " array([72960, 72961, 72962, ..., 89597, 89598, 89599], dtype=int64)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def consecutive(data, stepsize=1):\n",
    "    return np.split(data, np.where(np.diff(data) != stepsize)[0]+1)\n",
    "\n",
    "cons = consecutive(np.where(mask)[0])\n",
    "mins = np.array([arr.min() for arr in cons])\n",
    "maxs = np.array([arr.max() for arr in cons])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# len(wav)/22050\n",
    "# source_sr\n",
    "# import copy\n",
    "# cons2 = copy.copy(cons)\n",
    "\n",
    "\n",
    "allowed_pause = .4*22050\n",
    "joined_cons = [cons[0]]\n",
    "for con in cons[1:]:\n",
    "    # print(joined_cons)\n",
    "    if con[-1] - joined_cons[-1][-1] <= allowed_pause:\n",
    "        \n",
    "        prev = joined_cons.pop()\n",
    "        print(prev)\n",
    "        joined_cons.append(np.concatenate([prev, con]))\n",
    "    else:\n",
    "        joined_cons.append(con)\n",
    "\n",
    "joined_cons\n",
    "\n",
    "\n",
    "# consecutive(np.where((mins[1:]-maxs[:-1]) < .5*22050)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39359 63039 82239 89599] [ 7040 50880 72960 84160]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(maxs, mins) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2], dtype=int64)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (mins[1:]-maxs[:-1]) < .5*22050\n",
    "consecutive(np.where(a)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.00087523,  0.00051307,  0.00026207, ...,  0.00056371,\n",
       "        -0.00046649, -0.00013715], dtype=float32),\n",
       " array([-0.00150331, -0.00164158, -0.00083389, ..., -0.00511392,\n",
       "        -0.00460699, -0.00470363], dtype=float32),\n",
       " array([-0.00916225, -0.00969645, -0.00811925, ..., -0.00074893,\n",
       "        -0.00157988,  0.00079246], dtype=float32)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autovc.speaker_encoder.utils import split_audio\n",
    "split_audio(\"data/samples/mette_183.wav\", \"results\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2d64eaaf381fd06d7f37ec58cef8effe3342c0f8da6340727723fa2e10bc046"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
